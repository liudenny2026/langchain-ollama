import streamlit as st
from langchain_community.llms import Ollama
from langchain_core.messages import HumanMessage, AIMessage
import ollama
import os

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œé…ç½®
st.set_page_config(
    page_title="åŒ—äº¬éº¦å¼—ç‘ç§‘æŠ€æœ‰é™å…¬å¸æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ",
    page_icon="ğŸ¤–",
    layout="wide"
)

# é¡µé¢æ ‡é¢˜
st.title("ğŸ¤– åŒ—äº¬éº¦å¼—ç‘ç§‘æŠ€æœ‰é™å…¬å¸æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ")
st.markdown("---")

# åˆå§‹åŒ–å¯¹è¯å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# ä¾§è¾¹æ è®¾ç½®
st.sidebar.title("å¯¹è¯è®¾ç½®")

# OllamaæœåŠ¡å™¨åœ°å€è®¾ç½®
ollama_host = st.sidebar.text_input("OllamaæœåŠ¡å™¨åœ°å€", value="http://localhost:11434", help="è¾“å…¥OllamaæœåŠ¡å™¨åœ°å€")

# è¿æ¥æµ‹è¯•æŒ‰é’®
if st.sidebar.button("æµ‹è¯•è¿æ¥"):
    with st.sidebar:
        with st.spinner("æ­£åœ¨è¿æ¥OllamaæœåŠ¡å™¨..."):
            try:
                # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨è‡ªå®šä¹‰ä¸»æœº
                if ollama_host != "http://localhost:11434":
                    os.environ['OLLAMA_HOST'] = ollama_host
                else:
                    # å¦‚æœæ˜¯é»˜è®¤åœ°å€ï¼Œç¡®ä¿ç¯å¢ƒå˜é‡æœªè®¾ç½®æˆ–å·²æ¸…é™¤
                    if 'OLLAMA_HOST' in os.environ:
                        del os.environ['OLLAMA_HOST']
                
                # æµ‹è¯•è¿æ¥
                models = ollama.list()
                if isinstance(models, dict) and 'models' in models:
                    model_names = [model['name'] for model in models['models']]
                else:
                    model_names = [model['name'] for model in models]
                
                st.success(f"âœ… è¿æ¥æˆåŠŸï¼æ‰¾åˆ° {len(model_names)} ä¸ªæ¨¡å‹")
                
                # åœ¨ä¾§è¾¹æ æ˜¾ç¤ºæ¨¡å‹åˆ—è¡¨
                st.markdown("### å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
                for model in model_names:
                    st.markdown(f"- `{model}`")
                
                # æ›´æ–°session stateä»¥ä¾›åç»­ä½¿ç”¨
                st.session_state['ollama_connected'] = True
                st.session_state['available_models'] = model_names
                st.session_state['connection_error'] = None
                
            except Exception as e:
                st.error(f"âŒ è¿æ¥å¤±è´¥: {str(e)}")
                st.session_state['ollama_connected'] = False
                st.session_state['available_models'] = ['qwen3:1.7b']  # é»˜è®¤æ¨¡å‹
                st.session_state['connection_error'] = str(e)

# æ£€æŸ¥æ˜¯å¦å·²è¿æ¥å¹¶è·å–æ¨¡å‹åˆ—è¡¨
if 'available_models' not in st.session_state:
    # å°è¯•é»˜è®¤è¿æ¥
    try:
        models = ollama.list()
        if isinstance(models, dict) and 'models' in models:
            st.session_state['available_models'] = [model['name'] for model in models['models']]
        else:
            st.session_state['available_models'] = [model['name'] for model in models]
        st.session_state['ollama_connected'] = True
    except Exception:
        st.session_state['available_models'] = ['qwen3:1.7b']
        st.session_state['ollama_connected'] = False

# æ¨¡å‹é€‰æ‹©
selected_model = st.sidebar.selectbox(
    "é€‰æ‹©æ¨¡å‹", 
    st.session_state['available_models'], 
    index=0 if 'qwen3:1.7b' in st.session_state['available_models'] else 0
)

# æ¸©åº¦è®¾ç½®
temperature = st.sidebar.slider("æ¸©åº¦ (Temperature)", min_value=0.0, max_value=1.0, value=0.7, step=0.1, help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šéšæœº")

# æœ€å¤§é¢„æµ‹tokenæ•°
max_tokens = st.sidebar.slider("æœ€å¤§é¢„æµ‹Tokenæ•°", min_value=128, max_value=2048, value=512, step=64, help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦")

# æ¸…é™¤å¯¹è¯å†å²æŒ‰é’®
if st.sidebar.button("æ¸…é™¤å¯¹è¯å†å²"):
    st.session_state.messages = []
    st.rerun()

# æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
def check_ollama_service():
    try:
        # ä½¿ç”¨å½“å‰è®¾ç½®çš„ä¸»æœº
        if ollama_host != "http://localhost:11434":
            os.environ['OLLAMA_HOST'] = ollama_host
        else:
            if 'OLLAMA_HOST' in os.environ:
                del os.environ['OLLAMA_HOST']
        
        ollama.list()
        return True
    except Exception as e:
        st.error(f"OllamaæœåŠ¡ä¸å¯ç”¨: {e}")
        return False

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# è·å–ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„æ¶ˆæ¯..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # æ˜¾ç¤ºåŠ©æ‰‹æ€è€ƒè¿‡ç¨‹
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("ğŸ¤– æ­£åœ¨æ€è€ƒä¸­...")
        
        try:
            # æ£€æŸ¥OllamaæœåŠ¡
            if check_ollama_service():
                # ä½¿ç”¨LangChainçš„Ollamaé›†æˆ
                llm = Ollama(
                    model=selected_model,
                    temperature=temperature,
                    num_predict=max_tokens,
                    base_url=ollama_host if ollama_host != "http://localhost:11434" else None
                )
                
                # æå–å¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡
                context = ""
                for msg in st.session_state.messages:
                    if msg["role"] == "user":
                        context += f"Human: {msg['content']}\n"
                    else:
                        context += f"Assistant: {msg['content']}\n"
                
                # æ„å»ºå®Œæ•´æç¤º
                full_prompt = f"{context}\nHuman: {prompt}\nAssistant: "
                
                # è°ƒç”¨LangChainçš„Ollamaæ¨¡å‹
                response = llm.invoke(full_prompt)
                
                # æ˜¾ç¤ºå¹¶æ·»åŠ åˆ°å†å²è®°å½•
                message_placeholder.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
            else:
                error_msg = f"OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨åœ°å€å’Œè¿æ¥çŠ¶æ€ã€‚"
                message_placeholder.markdown(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
                
        except Exception as e:
            error_msg = f"å¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
            message_placeholder.markdown(error_msg)
            st.session_state.messages.append({"role": "assistant", "content": error_msg})

# æ˜¾ç¤ºå½“å‰å¯¹è¯è½®æ•°
st.sidebar.markdown(f"**å½“å‰å¯¹è¯è½®æ•°**: {len([m for m in st.session_state.messages if m['role'] == 'user'])}")

# æ·»åŠ å…³äºä¿¡æ¯
st.sidebar.markdown("---")
st.sidebar.markdown("### å…³äº")
st.sidebar.markdown("è¿™æ˜¯ä¸€ä¸ªåŸºäºLangChainå’ŒOllamaçš„å¤šè½®å¯¹è¯ç³»ç»Ÿã€‚")
st.sidebar.markdown("- ä½¿ç”¨Streamlitæ„å»ºç•Œé¢")
st.sidebar.markdown("- é›†æˆLangChainæ¡†æ¶")
st.sidebar.markdown("- æ”¯æŒå¤šè½®å¯¹è¯è®°å¿†")
st.sidebar.markdown("- å¯è‡ªå®šä¹‰OllamaæœåŠ¡å™¨åœ°å€")
st.sidebar.markdown("- æ”¯æŒæ¨¡å‹é€‰æ‹©")
st.sidebar.markdown("- å¯è°ƒèŠ‚ç”Ÿæˆå‚æ•°")